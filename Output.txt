> sink("run_log.txt", split = TRUE)
> 
> # ===============================
> # 1) Libraries
> # ===============================
> library(survey)
> library(pROC)
> library(PRROC)
> library(dplyr)
> library(caret)
> library(splines)
> library(ggeffects)
> 
> 
> # ===============================
> # 2) Load data
> # ===============================
> data.complete <- readRDS("data_complete_cleaned.rds")
> 
> cat("\n[INFO] Data loaded:\n")

[INFO] Data loaded:
> cat(" - Rows:", nrow(data.complete), "  Cols:", ncol(data.complete), "\n")
 - Rows: 8820   Cols: 44 
> cat(" - Cycles present:", paste(unique(as.character(data.complete$Cycle)), collapse=", "), "\n")
 - Cycles present: L, P 
> 
> 
> # ===============================
> # 3) Split by cycle
> # ===============================
> train_df <- subset(data.complete, Cycle == "P")  # 2017–2020
> test_df  <- subset(data.complete, Cycle == "L")  # 2021–2023
> 
> # Quartiles on train and cut both sets
> ibi_quartiles <- quantile(train_df$IBI, probs = c(0, .25, .5, .75, 1), na.rm = TRUE)
> cut_ibi <- function(x, breaks) cut(x, breaks = breaks, labels = c("Q1","Q2","Q3","Q4"), include.lowest = TRUE)
> 
> train_df$IBI_Category <- cut_ibi(train_df$IBI, ibi_quartiles)
> test_df$IBI_Category  <- cut_ibi(test_df$IBI,  ibi_quartiles)
> train_df$IBI_Category <- stats::relevel(train_df$IBI_Category, ref = "Q1")
> 
> # Target encodings
> train_df$CVD <- factor(train_df$CVD, levels = c("No","Yes"))
> test_df$CVD  <- factor(test_df$CVD,  levels = c("No","Yes"))
> train_df$CVD_num <- as.integer(train_df$CVD == "Yes")
> test_df$CVD_num  <- as.integer(test_df$CVD  == "Yes")
> 
> 
> # ===============================
> # 4) Build survey design on Training data
> # ===============================
> train_design <- svydesign(
+   id = ~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTMEC2YR,
+   nest = TRUE, survey.lonely.psu = "adjust",
+   data = train_df
+ )
> 
> 
> # ===============================
> # 5) Train weighted logistic models
> # ===============================
> cat("\n[FIT] Model 1: IBI only\n")

[FIT] Model 1: IBI only
> model1 <- svyglm(CVD ~ IBI_Category, design = train_design, family = quasibinomial("logit"))
> print(summary(model1))

Call:
svyglm(formula = CVD ~ IBI_Category, design = train_design, family = quasibinomial("logit"))

Survey design:
svydesign(id = ~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTMEC2YR, 
    nest = TRUE, survey.lonely.psu = "adjust", data = train_df)

Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
(Intercept)     -2.9030     0.1430 -20.302 9.74e-16 ***
IBI_CategoryQ2   0.5239     0.1868   2.805   0.0103 *  
IBI_CategoryQ3   1.0060     0.2088   4.818 8.19e-05 ***
IBI_CategoryQ4   0.8604     0.1642   5.240 2.95e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for quasibinomial family taken to be 1.000192)

Number of Fisher Scoring iterations: 5

> 
> cat("\n[FIT] Model 2: + Demographics\n")

[FIT] Model 2: + Demographics
> model2 <- svyglm(CVD ~ IBI_Category + Age + Gender + Ethnicity + Education,
+                  design = train_design, family = quasibinomial("logit"))
> print(summary(model2))

Call:
svyglm(formula = CVD ~ IBI_Category + Age + Gender + Ethnicity + 
    Education, design = train_design, family = quasibinomial("logit"))

Survey design:
svydesign(id = ~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTMEC2YR, 
    nest = TRUE, survey.lonely.psu = "adjust", data = train_df)

Coefficients:
                             Estimate Std. Error t value Pr(>|t|)    
(Intercept)                 -6.752491   0.327379 -20.626 2.02e-12 ***
IBI_CategoryQ2               0.320855   0.189063   1.697 0.110327    
IBI_CategoryQ3               0.710645   0.199648   3.559 0.002853 ** 
IBI_CategoryQ4               0.660267   0.181188   3.644 0.002399 ** 
Age                          0.074060   0.003954  18.728 8.16e-12 ***
GenderFemale                -0.404228   0.126933  -3.185 0.006155 ** 
EthnicityNon-Hispanic Black  0.844438   0.203119   4.157 0.000842 ***
EthnicityNon-Hispanic White  0.690410   0.186912   3.694 0.002167 ** 
EthnicityOther Muti-Racial   0.607525   0.269046   2.258 0.039274 *  
EducationAbove High School  -1.032850   0.214353  -4.818 0.000226 ***
EducationHigh School        -0.502951   0.232588  -2.162 0.047151 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for quasibinomial family taken to be 0.8787986)

Number of Fisher Scoring iterations: 6

> 
> cat("\n[FIT] Model 3: Fully adjusted\n")

[FIT] Model 3: Fully adjusted
> model3 <- svyglm(CVD ~ IBI_Category + Age + Gender + Ethnicity + Education +
+                    Smoking_status + Alcohol + BMI + Diabetes +
+                    TOTAL_CHOLESTEROL + HDL_CHOLESTEROL + Hypertension,
+                  design = train_design, family = quasibinomial("logit"))
> print(summary(model3))

Call:
svyglm(formula = CVD ~ IBI_Category + Age + Gender + Ethnicity + 
    Education + Smoking_status + Alcohol + BMI + Diabetes + TOTAL_CHOLESTEROL + 
    HDL_CHOLESTEROL + Hypertension, design = train_design, family = quasibinomial("logit"))

Survey design:
svydesign(id = ~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTMEC2YR, 
    nest = TRUE, survey.lonely.psu = "adjust", data = train_df)

Coefficients:
                              Estimate Std. Error t value Pr(>|t|)    
(Intercept)                  -5.769903   0.693627  -8.318  0.00041 ***
IBI_CategoryQ2                0.366133   0.195875   1.869  0.12054    
IBI_CategoryQ3                0.579009   0.180980   3.199  0.02401 *  
IBI_CategoryQ4                0.485103   0.198858   2.439  0.05869 .  
Age                           0.070033   0.004815  14.544 2.77e-05 ***
GenderFemale                 -0.085284   0.132625  -0.643  0.54852    
EthnicityNon-Hispanic Black   0.681859   0.186010   3.666  0.01451 *  
EthnicityNon-Hispanic White   0.702231   0.199304   3.523  0.01686 *  
EthnicityOther Muti-Racial    0.520814   0.283623   1.836  0.12575    
EducationAbove High School   -0.670462   0.232626  -2.882  0.03450 *  
EducationHigh School         -0.390622   0.224009  -1.744  0.14165    
Smoking_statusCurrent Smoker  0.694327   0.151701   4.577  0.00596 ** 
Smoking_statusFormer Smoker   0.368932   0.202624   1.821  0.12828    
AlcoholFrequent Drinker      -0.004705   0.144772  -0.033  0.97533    
AlcoholOccasional Drinker    -0.063942   0.099208  -0.645  0.54763    
BMI                           0.017581   0.009533   1.844  0.12446    
DiabetesYes                   0.670687   0.216354   3.100  0.02685 *  
DiabetesBorderline           -0.111198   0.373372  -0.298  0.77782    
TOTAL_CHOLESTEROL            -0.009493   0.001721  -5.515  0.00268 ** 
HDL_CHOLESTEROL              -0.005734   0.005832  -0.983  0.37065    
HypertensionYes               0.342737   0.126611   2.707  0.04243 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for quasibinomial family taken to be 0.8720977)

Number of Fisher Scoring iterations: 6

> 
> 
> # ===============================
> # 6) Predict on Test data (2021–2023)
> # ===============================
> test_df$pred1 <- predict(model1, newdata = test_df, type = "response")
> test_df$pred2 <- predict(model2, newdata = test_df, type = "response")
> test_df$pred3 <- predict(model3, newdata = test_df, type = "response")
> 
> # Example Predictions
> cat("\n[INFO] Example predictions (first 5 rows):\n")

[INFO] Example predictions (first 5 rows):
> print(head(test_df[, c("CVD","pred1","pred2","pred3")], 5))
  CVD     pred1       pred2       pred3
1  No 0.1304413 0.182977276 0.123546247
2  No 0.1147949 0.023185968 0.026429650
3  No 0.0520034 0.008685377 0.007501022
4  No 0.1147949 0.141511770 0.160804793
5  No 0.1304413 0.029428155 0.035979065
> 
> # Get Weighted Mean
> cat("\n[TEST prevalence]\n")

[TEST prevalence]
> prev <- with(test_df, stats::weighted.mean(CVD_num, WTMEC2YR, na.rm = TRUE))
> cat(" - Weighted prevalence of CVD (Yes):", round(prev, 3), "\n")
 - Weighted prevalence of CVD (Yes): 0.082 
> 
> # Weighted baseline accuracy -> majority class
> baseline_w_acc <- max(prev, 1 - prev)
> cat(" - Weighted baseline accuracy (majority class):", round(baseline_w_acc, 3), "\n")
 - Weighted baseline accuracy (majority class): 0.918 
> 
> 
> 
> # Function to get Weighted metrics at a given threshold
>   # Takes in predictions, target, weights, threshold
> weighted_metrics <- function(pred, y, w, thr) {
+   pred_cls <- ifelse(pred >= thr, 1L, 0L) # as int
+   keep <- is.finite(pred_cls) & is.finite(y) & is.finite(w)
+   pred_cls <- pred_cls[keep]; y <- y[keep]; w <- as.numeric(w[keep])
+   
+   TPw <- sum(w * (pred_cls == 1L) * (y == 1L), na.rm = TRUE)
+   FPw <- sum(w * (pred_cls == 1L) * (y == 0L), na.rm = TRUE)
+   TNw <- sum(w * (pred_cls == 0L) * (y == 0L), na.rm = TRUE)
+   FNw <- sum(w * (pred_cls == 0L) * (y == 1L), na.rm = TRUE)
+   Wtot <- TPw + FPw + TNw + FNw
+   
+   acc  <- (TPw + TNw) / Wtot
+   rec  <- ifelse((TPw + FNw) > 0, TPw / (TPw + FNw), NA_real_)  # sensitivity
+   spec <- ifelse((TNw + FPw) > 0, TNw / (TNw + FPw), NA_real_)
+   prec <- ifelse((TPw + FPw) > 0, TPw / (TPw + FPw), NA_real_)
+   f2   <- ifelse(is.na(prec) | is.na(rec) | (4*prec + rec) == 0,
+                  NA_real_, (5 * prec * rec) / (4 * prec + rec))
+   
+   tibble::tibble(
+     Threshold = thr,
+     Weighted_Accuracy    = round(acc, 3),
+     Weighted_Recall      = round(rec, 3),
+     Weighted_Specificity = round(spec, 3),
+     Weighted_Precision   = round(prec, 3),
+     Weighted_F2          = round(f2, 3)
+   )
+ }
> 
> 
> metrics1_test <- with(test_df, weighted_metrics(pred1, CVD_num, WTMEC2YR, 0.5))
> metrics2_test <- with(test_df, weighted_metrics(pred2, CVD_num, WTMEC2YR, 0.5))
> metrics3_test <- with(test_df, weighted_metrics(pred3, CVD_num, WTMEC2YR, 0.5))
> 
> metrics_5 <- dplyr::bind_rows(metrics1_test, metrics2_test, metrics3_test)
> cat("\n[WEIGHTED METRICS @ 0.5]\n"); print(metrics_5)

[WEIGHTED METRICS @ 0.5]
# A tibble: 3 × 6
  Threshold Weighted_Accuracy Weighted_Recall Weighted_Specificity Weighted_Precision Weighted_F2
      <dbl>             <dbl>           <dbl>                <dbl>              <dbl>       <dbl>
1       0.5             0.918           0                    1                 NA          NA    
2       0.5             0.916           0.033                0.995              0.373       0.041
3       0.5             0.92            0.105                0.992              0.553       0.125
> 
> 
> # ===============================
> # 7) ROC/AUC/Metrics (Test data) – weighted
> # ===============================
> roc1_test <- pROC::roc(test_df$CVD, test_df$pred1, levels = c("No","Yes"),
+                        weights = test_df$WTMEC2YR, quiet = TRUE)
> roc2_test <- pROC::roc(test_df$CVD, test_df$pred2, levels = c("No","Yes"),
+                        weights = test_df$WTMEC2YR, quiet = TRUE)
> roc3_test <- pROC::roc(test_df$CVD, test_df$pred3, levels = c("No","Yes"),
+                        weights = test_df$WTMEC2YR, quiet = TRUE)
> 
> auc1_test <- pROC::auc(roc1_test)
> auc2_test <- pROC::auc(roc2_test)
> auc3_test <- pROC::auc(roc3_test)
> 
> cat("\n[AUC on TEST]\n")

[AUC on TEST]
> cat(" - Model 1 (IBI only):      ", round(auc1_test, 3), "\n")
 - Model 1 (IBI only):       0.528 
> cat(" - Model 2 (Demographics):  ", round(auc2_test, 3), "\n")
 - Model 2 (Demographics):   0.775 
> cat(" - Model 3 (Full):          ", round(auc3_test, 3), "\n")
 - Model 3 (Full):           0.817 
> 
> 
> # ===============================
> # 8) Train-set ROC/AUC (unweighted)
> # ===============================
> train_df$pred1_tr <- predict(model1, type = "response")
> train_df$pred2_tr <- predict(model2, type = "response")
> train_df$pred3_tr <- predict(model3, type = "response")
> 
> roc1_train <- roc(train_df$CVD, train_df$pred1_tr, levels = c("No","Yes"))
Setting direction: controls < cases
> roc2_train <- roc(train_df$CVD, train_df$pred2_tr, levels = c("No","Yes"))
Setting direction: controls < cases
> roc3_train <- roc(train_df$CVD, train_df$pred3_tr, levels = c("No","Yes"))
Setting direction: controls < cases
> 
> results_compare <- data.frame(
+   Model = c("Model 1","Model 2","Model 3"),
+   Train_AUC = c(auc(roc1_train), auc(roc2_train), auc(roc3_train)),
+   Test_AUC  = c(auc1_test,       auc2_test,       auc3_test)
+ )
> 
> results_compare_print <- dplyr::mutate(results_compare,
+                                        dplyr::across(where(is.numeric), ~ round(.x, 3)))
> cat("\n[AUC TRAIN vs TEST]\n"); print(results_compare_print)

[AUC TRAIN vs TEST]
    Model Train_AUC Test_AUC
1 Model 1     0.584    0.528
2 Model 2     0.801    0.775
3 Model 3     0.826    0.817
> write.csv(results_compare_print, "auc_train_vs_test.csv", row.names = FALSE)
> cat("[FILE SAVED] auc_train_vs_test.csv\n")
[FILE SAVED] auc_train_vs_test.csv
> 
> # ===============================
> # 9) Plot Test ROC curves
> # ===============================
> # All 3models 
> plot(roc1_test, col = "red", main = "ROC: Train (2017–2020) → Test (2021–2023)",
+      xlab = "1 - Specificity", ylab = "Sensitivity", lwd = 2)
> lines(roc2_test, col = "blue", lwd = 2)
> lines(roc3_test, col = "green", lwd = 2)
> legend("bottomright",
+        legend = c(paste("Model 1:", round(auc1_test, 3)),
+                   paste("Model 2:", round(auc2_test, 3)),
+                   paste("Model 3:", round(auc3_test, 3))),
+        col = c("red","blue","green"), lty = 1, lwd = 2, cex = 0.85)
> cat("\n[INFO] ROC plot displayed in Plots panel\n")

[INFO] ROC plot displayed in Plots panel
> 
> # Save PNG
> png("ROC_Curves_TrainToTest.png", width = 800, height = 600)
> plot(roc1_test, col = "red", main = "ROC: Train (2017–2020) → Test (2021–2023)",
+      xlab = "1 - Specificity", ylab = "Sensitivity", lwd = 2)
> lines(roc2_test, col = "blue", lwd = 2)
> lines(roc3_test, col = "green", lwd = 2)
> legend("bottomright",
+        legend = c(paste("Model 1:", round(auc1_test, 3)),
+                   paste("Model 2:", round(auc2_test, 3)),
+                   paste("Model 3:", round(auc3_test, 3))),
+        col = c("red","blue","green"), lty = 1, lwd = 2, cex = 0.85)
> dev.off()
RStudioGD 
        2 
> cat("[FILE SAVED] ROC_Curves_TrainToTest.png\n")
[FILE SAVED] ROC_Curves_TrainToTest.png
> 
> 
> 
> # Weighted precision–recall curve for Model 3
> wpr_auc <- PRROC::pr.curve(
+   scores.class0 = test_df$pred3[test_df$CVD_num == 0],
+   scores.class1 = test_df$pred3[test_df$CVD_num == 1],
+   weights.class0 = test_df$WTMEC2YR[test_df$CVD_num == 0],
+   weights.class1 = test_df$WTMEC2YR[test_df$CVD_num == 1],
+   curve = FALSE # took up too much memory
+ )$auc.integral
> cat("Weighted PR-AUC (Model 3):", round(wpr_auc, 3), "\n")
Weighted PR-AUC (Model 3): 0.824 
> 
> 
> # ===============================
> # 10) Continuous IBI and model  with splines
> # ===============================
> 
> # Continuous IBI
> model3_cont <- svyglm(CVD ~ IBI + Age + Gender + Ethnicity + Education +
+                         Smoking_status + Alcohol + BMI + Diabetes +
+                         TOTAL_CHOLESTEROL + HDL_CHOLESTEROL + Hypertension,
+                       design = train_design, family = quasibinomial("logit"))
> 
> # Splines for IBI
> model3_spline <- svyglm(CVD ~ ns(IBI, df = 3) + Age + Gender + Ethnicity + Education +
+                           Smoking_status + Alcohol + BMI + Diabetes +
+                           TOTAL_CHOLESTEROL + HDL_CHOLESTEROL + Hypertension,
+                         design = train_design, family = quasibinomial("logit"))
> 
> # Compare TEST AUCs for these specs
> test_df$pred3_cont   <- predict(model3_cont,   newdata=test_df, type="response")
> test_df$pred3_spline <- predict(model3_spline, newdata=test_df, type="response")
> 
> roc3_cont   <- pROC::roc(test_df$CVD, test_df$pred3_cont,   levels=c("No","Yes"), weights=test_df$WTMEC2YR, quiet=TRUE)
> roc3_spline <- pROC::roc(test_df$CVD, test_df$pred3_spline, levels=c("No","Yes"), weights=test_df$WTMEC2YR, quiet=TRUE)
> 
> cat("\n[Model 3 variants — AUC on TEST]\n")

[Model 3 variants — AUC on TEST]
> cat("  IBI continuous: ", round(pROC::auc(roc3_cont),   3), "\n")
  IBI continuous:  0.821 
> cat("  IBI spline:     ", round(pROC::auc(roc3_spline), 3), "\n")
  IBI spline:      0.818 
> 
> 
> 
> # ===============================
> # 11)  Dose–response for IBI
> # ===============================
> 
> 
> lo <- as.numeric(quantile(train_df$IBI, 0.01, na.rm = TRUE))
> hi <- as.numeric(quantile(train_df$IBI, 0.99, na.rm = TRUE))
> step <- (hi - lo) / 200  # ~200 points
> 
> gg_ibi <- ggpredict(
+   model3_cont,
+   terms = sprintf("IBI [%.6f:%.6f by=%.6f]", lo, hi, step)
+ )
> 
> plot(gg_ibi) +
+   theme_minimal() +
+   labs(
+     title = "Continuous IBI dose–response (trimmed to 1st–99th pct)",
+     x = "IBI",
+     y = "Predicted Probability of CVD"
+   )
> 
> 
> # ===============================
> # 12) Save models & other objects
> # ===============================
> saveRDS(model1, "model1_ibi_only_trainP.rds")
> saveRDS(model2, "model2_demographic_adjusted_trainP.rds")
> saveRDS(model3, "model3_fully_adjusted_trainP.rds")
> saveRDS(roc1_test, "roc_model1_testL.rds")
> saveRDS(roc2_test, "roc_model2_testL.rds")
> saveRDS(roc3_test, "roc_model3_testL.rds")
> saveRDS(ibi_quartiles, "ibi_quartiles_trainP.rds")
> readr::write_csv(
+   dplyr::transmute(test_df, SEQN, CVD, CVD_num, pred1, pred2, pred3, WTMEC2YR),
+   "test_predictions_L.csv"
+ )
> 
> cat("\n[FILES SAVED]\n",
+     "- model1_ibi_only_trainP.rds\n",
+     "- model2_demographic_adjusted_trainP.rds\n",
+     "- model3_fully_adjusted_trainP.rds\n",
+     "- roc_model1_testL.rds\n",
+     "- roc_model2_testL.rds\n",
+     "- roc_model3_testL.rds\n",
+     "- ibi_quartiles_trainP.rds\n",
+     "- test_predictions_L.csv\n"
+     )

[FILES SAVED]
 - model1_ibi_only_trainP.rds
 - model2_demographic_adjusted_trainP.rds
 - model3_fully_adjusted_trainP.rds
 - roc_model1_testL.rds
 - roc_model2_testL.rds
 - roc_model3_testL.rds
 - ibi_quartiles_trainP.rds
 - test_predictions_L.csv
> 
> 
> 
> writeLines(capture.output(sessionInfo()), "sessionInfo.txt")
> cat("[FILE SAVED] sessionInfo.txt\n")
[FILE SAVED] sessionInfo.txt
> 
> utils::savehistory("Rhistory_this_run.Rhistory")
> 
> 
> # End logging 
> sink()