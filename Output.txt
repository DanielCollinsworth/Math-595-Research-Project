
> library(pROC)
Type 'citation("pROC")' for a citation.

Attaching package: ‘pROC’

The following objects are masked from ‘package:stats’:

    cov, smooth, var

> library(PRROC)
Loading required package: rlang
> library(dplyr)

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union

> library(caret)
Loading required package: ggplot2
Loading required package: lattice

Attaching package: ‘caret’

The following object is masked from ‘package:survival’:

    cluster

> library(splines)
> library(ggeffects)
> 
> 
> # ===============================
> # 2) Load data
> # ===============================
> data.complete <- readRDS("data_complete_cleaned.rds")
> # str(data.complete)
> # head(data.complete)
> 
> cat("\n[INFO] Data loaded:\n")

[INFO] Data loaded:
> cat(" - Rows:", nrow(data.complete), "  Cols:", ncol(data.complete), "\n")
 - Rows: 10336   Cols: 41 
> cat(" - Cycle L Rows: ", sum(data.complete$Cycle == "L"))
 - Cycle L Rows:  4218> cat(" - Cycle P Rows: ", sum(data.complete$Cycle == "P"))
 - Cycle P Rows:  6118> 
> # ===============================
> # 3) Split by cycle & build IBI quartiles on TRAIN
> # ===============================
> train_df <- subset(data.complete, Cycle == "P")  # 2017–2020
> test_df  <- subset(data.complete, Cycle == "L")  # 2021–2023
> 
> 
> 
> # Quartiles on  IBI
> ibi_quartiles <- quantile(train_df$IBI, probs = c(0, .25, .5, .75, 1), na.rm = TRUE)
> 
> # function to cut into quartiles
> cut_ibi <- function(x, breaks) {
+   cut(x, breaks = breaks, labels = c("Q1","Q2","Q3","Q4"),
+       include.lowest = TRUE, right = TRUE, ordered_result = TRUE)
+ }
> 
> train_df$IBI_Category <- cut_ibi(train_df$IBI, ibi_quartiles)
> test_df$IBI_Category  <- cut_ibi(test_df$IBI,  ibi_quartiles)
> 
> # factor and revel quartiles
> train_df$IBI_Category <- factor(as.character(train_df$IBI_Category),
+                                 levels = c("Q1","Q2","Q3","Q4"), ordered = FALSE)
> train_df$IBI_Category <- relevel(train_df$IBI_Category, ref = "Q1")
> 
> 
> test_df$IBI_Category <- factor(as.character(test_df$IBI_Category),
+                                levels = c("Q1","Q2","Q3","Q4"), ordered = FALSE)
> test_df$IBI_Category <- relevel(test_df$IBI_Category, ref = "Q1")
> 
> 
> # Quartiles as Numeric 1-4 
> train_df$IBI_QuartileNum <- as.numeric(train_df$IBI_Category)
> test_df$IBI_QuartileNum  <- as.numeric(test_df$IBI_Category)
> 
> 
> # Target encodings
> train_df$CVD <- factor(train_df$CVD, levels = c("No","Yes"))
> test_df$CVD  <- factor(test_df$CVD,  levels = c("No","Yes"))
> train_df$CVD_num <- as.integer(train_df$CVD == "Yes")
> test_df$CVD_num  <- as.integer(test_df$CVD  == "Yes")
> 
> # size and cutoffs
> cat("\n[SECTION 3 SUMMARY]\n")

[SECTION 3 SUMMARY]
> cat(" - Train rows (Cycle P):", nrow(train_df), "\n")
 - Train rows (Cycle P): 6118 
> cat(" - Test  rows (Cycle L):", nrow(test_df),  "\n")
 - Test  rows (Cycle L): 4218 
> cat(" - Quartile cutpoints (IBI, raw on TRAIN):\n"); print(ibi_quartiles)
 - Quartile cutpoints (IBI, raw on TRAIN):
          0%          25%          50%          75%         100% 
4.418182e-02 1.484053e+00 3.737348e+00 9.079142e+00 2.050837e+03 
> 
> 
> # ===============================
> # 4) Build survey design on Training data
> # ===============================
> train_design <- svydesign(
+   id = ~SDMVPSU,
+   strata = ~SDMVSTRA,
+   weights  = ~WTMEC2YR,
+   nest  = TRUE,
+   survey.lonely.psu = "adjust",
+   data = train_df
+ )
> 
> 
> # ===============================
> # 5) build Models and find OPs and CIs
> # ===============================
> cat("\n[FIT] Model 1: IBI only\n")

[FIT] Model 1: IBI only
> model1 <- svyglm(CVD ~ IBI_Category, design = train_design, family = binomial("logit"))
Warning message:
In eval(family$initialize) : non-integer #successes in a binomial glm!
> print(summary(model1))

Call:
svyglm(formula = CVD ~ IBI_Category, design = train_design, family = binomial("logit"))

Survey design:
svydesign(id = ~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTMEC2YR, 
    nest = TRUE, survey.lonely.psu = "adjust", data = train_df)

Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
(Intercept)     -2.9307     0.1475 -19.868 1.53e-15 ***
IBI_CategoryQ2   0.6247     0.1785   3.499  0.00203 ** 
IBI_CategoryQ3   0.9767     0.1938   5.039 4.79e-05 ***
IBI_CategoryQ4   1.0483     0.1699   6.170 3.29e-06 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1.000163)

Number of Fisher Scoring iterations: 5

> 
> cat("\n[FIT] Model 2: + Demographics\n")

[FIT] Model 2: + Demographics
> model2 <- svyglm(CVD ~ IBI_Category + Age + Gender + Ethnicity + Education,
+                  design = train_design, family = binomial("logit"))
Warning message:
In eval(family$initialize) : non-integer #successes in a binomial glm!
> print(summary(model2))

Call:
svyglm(formula = CVD ~ IBI_Category + Age + Gender + Ethnicity + 
    Education, design = train_design, family = binomial("logit"))

Survey design:
svydesign(id = ~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTMEC2YR, 
    nest = TRUE, survey.lonely.psu = "adjust", data = train_df)

Coefficients:
                             Estimate Std. Error t value Pr(>|t|)    
(Intercept)                 -7.135709   0.293461 -24.316 1.83e-13 ***
IBI_CategoryQ2               0.387060   0.182026   2.126 0.050480 .  
IBI_CategoryQ3               0.663576   0.187133   3.546 0.002933 ** 
IBI_CategoryQ4               0.847432   0.188383   4.498 0.000425 ***
Age                          0.075272   0.003967  18.974 6.76e-12 ***
GenderFemale                -0.493452   0.113951  -4.330 0.000594 ***
EthnicityHispanic           -0.595320   0.174720  -3.407 0.003899 ** 
EthnicityNon-Hispanic Black  0.150119   0.126935   1.183 0.255356    
EthnicityOther Muti-Racial   0.170755   0.248996   0.686 0.503317    
EducationBelow High School   0.994116   0.228841   4.344 0.000578 ***
EducationHigh School         0.493891   0.170876   2.890 0.011212 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 0.8561514)

Number of Fisher Scoring iterations: 6

> 
> cat("\n[FIT] Model 3: Fully adjusted\n")

[FIT] Model 3: Fully adjusted
> model3 <- svyglm(CVD ~ IBI_Category + Age + Gender + Ethnicity + Education +
+                    Smoking_status + Alcohol + BMI + Diabetes +
+                    TOTAL_CHOLESTEROL + HDL_CHOLESTEROL + Hypertension,
+                  design = train_design, family = binomial("logit"))
Warning message:
In eval(family$initialize) : non-integer #successes in a binomial glm!
> print(summary(model3))

Call:
svyglm(formula = CVD ~ IBI_Category + Age + Gender + Ethnicity + 
    Education + Smoking_status + Alcohol + BMI + Diabetes + TOTAL_CHOLESTEROL + 
    HDL_CHOLESTEROL + Hypertension, design = train_design, family = binomial("logit"))

Survey design:
svydesign(id = ~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTMEC2YR, 
    nest = TRUE, survey.lonely.psu = "adjust", data = train_df)

Coefficients:
                              Estimate Std. Error t value Pr(>|t|)    
(Intercept)                  -5.760154   0.512440 -11.241 9.73e-05 ***
IBI_CategoryQ2                0.411489   0.186062   2.212  0.07795 .  
IBI_CategoryQ3                0.543978   0.166330   3.270  0.02219 *  
IBI_CategoryQ4                0.639112   0.195352   3.272  0.02216 *  
Age                           0.069875   0.004301  16.248 1.61e-05 ***
GenderFemale                 -0.201157   0.124854  -1.611  0.16807    
EthnicityHispanic            -0.612506   0.186667  -3.281  0.02192 *  
EthnicityNon-Hispanic Black  -0.027628   0.141112  -0.196  0.85248    
EthnicityOther Muti-Racial    0.030552   0.262396   0.116  0.91184    
EducationBelow High School    0.671820   0.224326   2.995  0.03028 *  
EducationHigh School          0.277627   0.150269   1.848  0.12394    
Smoking_statusCurrent Smoker  0.569314   0.153397   3.711  0.01384 *  
Smoking_statusFormer Smoker   0.320015   0.193136   1.657  0.15843    
AlcoholFrequent Drinker      -0.075121   0.126088  -0.596  0.57726    
AlcoholOccasional Drinker    -0.091408   0.094863  -0.964  0.37951    
BMI                           0.016196   0.007353   2.203  0.07881 .  
DiabetesYes                   0.769041   0.149982   5.128  0.00368 ** 
DiabetesBorderline            0.083039   0.295404   0.281  0.78989    
TOTAL_CHOLESTEROL            -0.009033   0.001579  -5.721  0.00228 ** 
HDL_CHOLESTEROL              -0.004495   0.005056  -0.889  0.41471    
HypertensionYes               0.335455   0.127043   2.640  0.04595 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 0.8631825)

Number of Fisher Scoring iterations: 6

> 
> 
> 
> cat("\n[FIT] Model 3: Fully adjusted\n")

[FIT] Model 3: Fully adjusted
> model3_Q_Num <- svyglm(CVD ~ IBI_QuartileNum + Age + Gender + Ethnicity + Education +
+                    Smoking_status + Alcohol + BMI + Diabetes +
+                    TOTAL_CHOLESTEROL + HDL_CHOLESTEROL + Hypertension,
+                  design = train_design, family = binomial("logit"))
Warning message:
In eval(family$initialize) : non-integer #successes in a binomial glm!
> print(summary(model3_Q_Num))

Call:
svyglm(formula = CVD ~ IBI_QuartileNum + Age + Gender + Ethnicity + 
    Education + Smoking_status + Alcohol + BMI + Diabetes + TOTAL_CHOLESTEROL + 
    HDL_CHOLESTEROL + Hypertension, design = train_design, family = binomial("logit"))

Survey design:
svydesign(id = ~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTMEC2YR, 
    nest = TRUE, survey.lonely.psu = "adjust", data = train_df)

Coefficients:
                              Estimate Std. Error t value Pr(>|t|)    
(Intercept)                  -5.807182   0.530230 -10.952 1.17e-05 ***
IBI_QuartileNum               0.186097   0.058050   3.206 0.014944 *  
Age                           0.070196   0.004222  16.626 6.96e-07 ***
GenderFemale                 -0.200669   0.122843  -1.634 0.146374    
EthnicityHispanic            -0.610009   0.182588  -3.341 0.012404 *  
EthnicityNon-Hispanic Black  -0.031117   0.141047  -0.221 0.831690    
EthnicityOther Muti-Racial    0.025240   0.263828   0.096 0.926466    
EducationBelow High School    0.662792   0.227256   2.917 0.022452 *  
EducationHigh School          0.281409   0.151510   1.857 0.105620    
Smoking_statusCurrent Smoker  0.560769   0.153387   3.656 0.008114 ** 
Smoking_statusFormer Smoker   0.314616   0.192965   1.630 0.147033    
AlcoholFrequent Drinker      -0.078065   0.127674  -0.611 0.560235    
AlcoholOccasional Drinker    -0.090718   0.098203  -0.924 0.386338    
BMI                           0.015638   0.007235   2.161 0.067460 .  
DiabetesYes                   0.761064   0.144030   5.284 0.001143 ** 
DiabetesBorderline            0.084976   0.295674   0.287 0.782130    
TOTAL_CHOLESTEROL            -0.008954   0.001571  -5.699 0.000736 ***
HDL_CHOLESTEROL              -0.004895   0.005142  -0.952 0.372818    
HypertensionYes               0.338672   0.126454   2.678 0.031621 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 0.8625957)

Number of Fisher Scoring iterations: 6

> 
> 
> 
> # ===============================
> # 6) Predict on Test data (2021–2023)
> # ===============================
> test_df$pred1 <- predict(model1, newdata = test_df, type = "response")
> test_df$pred2 <- predict(model2, newdata = test_df, type = "response")
> test_df$pred3 <- predict(model3, newdata = test_df, type = "response")
> 
> # Some Example Predictions
> cat("\n[INFO] Example predictions (first 5 rows):\n")

[INFO] Example predictions (first 5 rows):
> print(head(test_df[, c("CVD","pred1","pred2","pred3")], 5))
  CVD      pred1       pred2       pred3
1  No 0.12411038 0.181805429 0.117152055
2  No 0.13211085 0.027353484 0.034445860
3  No 0.05065512 0.009213036 0.007805833
4  No 0.13211085 0.159330996 0.172539861
5  No 0.13211085 0.229421498 0.249264578
> 
> # Get Weighted Mean
> cat("\n[TEST prevalence]\n")

[TEST prevalence]
> prev <- with(test_df, stats::weighted.mean(CVD_num, WTMEC2YR, na.rm = TRUE))
> cat(" - Weighted prevalence of CVD (Yes):", round(prev, 3), "\n")
 - Weighted prevalence of CVD (Yes): 0.087 
> 
> 
> # Weighted baseline accuracy, i.e. majority class % of total
> baseline_w_acc <- max(prev, 1 - prev)
> cat(" - Weighted baseline accuracy (majority class):", round(baseline_w_acc, 3), "\n")
 - Weighted baseline accuracy (majority class): 0.913 
> 
> 
> 
> # Function to get performance metrics at a given threshold
>   # Takes in predictions, target, weights, threshold
> weighted_metrics <- function(pred, y, w, thr) {
+   pred_cls <- ifelse(pred >= thr, 1L, 0L)
+   keep <- is.finite(pred_cls) & is.finite(y) & is.finite(w)
+   pred_cls <- pred_cls[keep]; y <- y[keep]; w <- as.numeric(w[keep])
+   
+   TPw <- sum(w * (pred_cls == 1L) * (y == 1L), na.rm = TRUE)
+   FPw <- sum(w * (pred_cls == 1L) * (y == 0L), na.rm = TRUE)
+   TNw <- sum(w * (pred_cls == 0L) * (y == 0L), na.rm = TRUE)
+   FNw <- sum(w * (pred_cls == 0L) * (y == 1L), na.rm = TRUE)
+   Wtot <- TPw + FPw + TNw + FNw
+   
+   acc  <- (TPw + TNw) / Wtot
+   sens <- ifelse((TPw + FNw) > 0, TPw / (TPw + FNw), NA_real_)
+   spec <- ifelse((TNw + FPw) > 0, TNw / (TNw + FPw), NA_real_)
+   ppv  <- ifelse((TPw + FPw) > 0, TPw / (TPw + FPw), NA_real_)
+   npv  <- ifelse((TNw + FNw) > 0, TNw / (TNw + FNw), NA_real_)
+   
+   lr_pos_raw <- ifelse((1 - spec) > 0, sens / (1 - spec), NA_real_)  
+   lr_neg_raw <- ifelse(spec > 0, (1 - sens) / spec, NA_real_) 
+   
+   f2 <- ifelse(is.na(ppv) | is.na(sens) | (4*ppv + sens) == 0,
+                NA_real_, (5 * ppv * sens) / (4 * ppv + sens))
+   
+   tibble::tibble(
+     Threshold = thr,
+     Weighted_Accuracy  = round(acc, 3),
+     Weighted_Recall = round(sens, 3),
+     Weighted_Specificity = round(spec, 3),
+     Weighted_Precision = round(ppv, 3),
+     Weighted_NPV  = round(npv, 3),
+     LR_Positive  = round(lr_pos_raw, 3),
+     LR_Negative = round(lr_neg_raw, 3),
+     Weighted_F2 = round(f2, 3)
+   )
+ }
> 
> 
> metrics1_test <- with(test_df, weighted_metrics(pred1, CVD_num, WTMEC2YR, 0.5))
> metrics2_test <- with(test_df, weighted_metrics(pred2, CVD_num, WTMEC2YR, 0.5))
> metrics3_test <- with(test_df, weighted_metrics(pred3, CVD_num, WTMEC2YR, 0.5))
> 
> metrics_05 <- dplyr::bind_rows(metrics1_test, metrics2_test, metrics3_test)
> cat("\n[WEIGHTED METRICS @ 0.5]\n"); print(metrics_05)

[WEIGHTED METRICS @ 0.5]
# A tibble: 3 × 9
  Threshold Weighted_Accuracy Weighted_Recall Weighted_Specificity Weighted_Precision Weighted_NPV LR_Positive LR_Negative
      <dbl>             <dbl>           <dbl>                <dbl>              <dbl>        <dbl>       <dbl>       <dbl>
1       0.5             0.913           0                    1                 NA            0.913       NA          1    
2       0.5             0.913           0.051                0.994              0.466        0.917        9.20       0.955
3       0.5             0.915           0.134                0.99               0.553        0.923       13.0        0.875
# ℹ 1 more variable: Weighted_F2 <dbl>
> 
> # ===============================
> # 7) ROC/AUC/Metrics (Test data)
> # ===============================
> roc1_test <- pROC::roc(test_df$CVD, test_df$pred1, levels = c("No","Yes"),
+                        weights = test_df$WTMEC2YR, quiet = TRUE)
> roc2_test <- pROC::roc(test_df$CVD, test_df$pred2, levels = c("No","Yes"),
+                        weights = test_df$WTMEC2YR, quiet = TRUE)
> roc3_test <- pROC::roc(test_df$CVD, test_df$pred3, levels = c("No","Yes"),
+                        weights = test_df$WTMEC2YR, quiet = TRUE)
> 
> auc1_test <- pROC::auc(roc1_test)
> auc2_test <- pROC::auc(roc2_test)
> auc3_test <- pROC::auc(roc3_test)
> 
> cat("\n[AUC on TEST]\n")

[AUC on TEST]
> cat(" - Model 1 (IBI only):      ", round(auc1_test, 3), "\n")
 - Model 1 (IBI only):       0.552 
> cat(" - Model 2 (Demographics):  ", round(auc2_test, 3), "\n")
 - Model 2 (Demographics):   0.785 
> cat(" - Model 3 (Full):          ", round(auc3_test, 3), "\n")
 - Model 3 (Full):           0.825 
> 
> 
> 
> # 7b) Youden thresholds 
> 
> # function to find youden info
> youden_info <- function(roc_obj) {
+   # Returns threshold, sensitivity, specificity, and J = sens + spec - 1
+   cs <- pROC::coords(
+     roc_obj,
+     x = "best",
+     best.method = "youden",
+     ret = c("threshold", "sensitivity", "specificity"),
+     transpose = FALSE
+   )
+   thr  <- as.numeric(cs[1, "threshold"])
+   sens <- as.numeric(cs[1, "sensitivity"])
+   spec <- as.numeric(cs[1, "specificity"])
+   data.frame(
+     Threshold = thr,
+     Sensitivity = sens,
+     Specificity = spec,
+     J = sens + spec - 1
+   )
+ }
> 
> youden1 <- youden_info(roc1_test)
> youden2 <- youden_info(roc2_test)
> youden3 <- youden_info(roc3_test)
> 
> youden_tbl <- dplyr::bind_rows(
+   dplyr::mutate(youden1, Model = "Model 1", .before = 1),
+   dplyr::mutate(youden2, Model = "Model 2", .before = 1),
+   dplyr::mutate(youden3, Model = "Model 3", .before = 1)
+ ) |>
+   dplyr::mutate(dplyr::across(where(is.numeric), ~ round(.x, 4)))
> 
> cat("\n[YOUDEN THRESHOLDS on TEST]\n"); print(youden_tbl)

[YOUDEN THRESHOLDS on TEST]
    Model Threshold Sensitivity Specificity      J
1 Model 1    0.0706      0.8282      0.2538 0.0820
2 Model 2    0.1237      0.7702      0.6621 0.4323
3 Model 3    0.1448      0.7619      0.7553 0.5172
> 
> # Evaluate weighted metrics at each model's Youden threshold
> metrics1_youden <- with(test_df, weighted_metrics(pred1, CVD_num, WTMEC2YR, thr = youden1$Threshold))
> metrics2_youden <- with(test_df, weighted_metrics(pred2, CVD_num, WTMEC2YR, thr = youden2$Threshold))
> metrics3_youden <- with(test_df, weighted_metrics(pred3, CVD_num, WTMEC2YR, thr = youden3$Threshold))
> 
> metrics_youden <- dplyr::bind_rows(
+   dplyr::mutate(metrics1_youden, Model = "Model 1", .before = 1),
+   dplyr::mutate(metrics2_youden, Model = "Model 2", .before = 1),
+   dplyr::mutate(metrics3_youden, Model = "Model 3", .before = 1)
+ )
> 
> cat("\n[WEIGHTED METRICS @ YOUDEN THRESHOLD]\n"); print(metrics_youden)

[WEIGHTED METRICS @ YOUDEN THRESHOLD]
# A tibble: 3 × 10
  Model   Threshold Weighted_Accuracy Weighted_Recall Weighted_Specificity Weighted_Precision Weighted_NPV LR_Positive LR_Negative
  <chr>       <dbl>             <dbl>           <dbl>                <dbl>              <dbl>        <dbl>       <dbl>       <dbl>
1 Model 1    0.0706             0.321           0.824                0.273              0.097        0.942        1.13       0.644
2 Model 2    0.124              0.755           0.737                0.757              0.223        0.968        3.03       0.348
3 Model 3    0.145              0.815           0.722                0.824              0.28         0.969        4.10       0.338
# ℹ 1 more variable: Weighted_F2 <dbl>
> 
> 
> 
> 
> # 7c) Model 3 cutoff calibration: Youden, Rule-in, Rule-out, Closest topleft
> 
> # Helper - ROC coords
> roc_all <- pROC::coords(
+   roc3_test,
+   x = "all",
+   ret = c("threshold", "sensitivity", "specificity"),
+   transpose = FALSE
+ )
> roc_all <- dplyr::mutate(
+   as.data.frame(roc_all),
+   J = sensitivity + specificity - 1,
+   dist_topleft = sqrt((1 - sensitivity)^2 + (1 - specificity)^2) # distance to (0,1)
+ )
> 
> # Get a row for each different index 
> 
> # 1) Youden J (maximize J)
> youden_row <- roc_all[which.max(roc_all$J), , drop = FALSE]
> 
> # 2) Rule-in, choose the max sensitivity with specificity >= 0.95
> rulein_pool <- dplyr::filter(roc_all, specificity >= 0.95)
> rulein_row <- if (nrow(rulein_pool)) {
+   rulein_pool[which.max(rulein_pool$sensitivity), , drop = FALSE]
+ } else {
+   # if none >= 0.95 take the row with maximum specificity
+   roc_all[which.max(roc_all$specificity), , drop = FALSE]
+ }
> 
> # 3) Rule-out, choose the with max specificity with sensitivity >= 0.95
> ruleout_pool <- dplyr::filter(roc_all, sensitivity >= 0.95)
> ruleout_row <- if (nrow(ruleout_pool)) {
+   ruleout_pool[which.max(ruleout_pool$specificity), , drop = FALSE]
+ } else {
+   # if none >= 0.95 take the row with maximum sensitivity
+   roc_all[which.max(roc_all$sensitivity), , drop = FALSE]
+ }
> 
> # 4) Closest to (0,1)
> closest_row <- roc_all[which.min(roc_all$dist_topleft), , drop = FALSE]
> 
> # table of all 4 thresholds
> thr_tbl_m3 <- dplyr::bind_rows(
+   dplyr::mutate(youden_row,  Index = "Youden J",        .before = 1),
+   dplyr::mutate(rulein_row,  Index = "Rule-in (Spec≥0.95)", .before = 1),
+   dplyr::mutate(ruleout_row, Index = "Rule-out (Sens≥0.95)", .before = 1),
+   dplyr::mutate(closest_row, Index = "Closest to (0,1)", .before = 1)
+ ) |>
+   dplyr::transmute(
+     Index,
+     Threshold = as.numeric(threshold),
+     Sensitivity = sensitivity,
+     Specificity = specificity,
+     J = J
+   ) |>
+   dplyr::mutate(dplyr::across(where(is.numeric), ~ round(.x, 4)))
> 
> cat("\n[MODEL 3: Candidate thresholds]\n"); print(thr_tbl_m3)

[MODEL 3: Candidate thresholds]
                 Index Threshold Sensitivity Specificity      J
1             Youden J    0.1448      0.7619      0.7553 0.5172
2  Rule-in (Spec≥0.95)    0.3650      0.3023      0.9502 0.2525
3 Rule-out (Sens≥0.95)    0.0399      0.9503      0.4402 0.3905
4     Closest to (0,1)    0.1448      0.7619      0.7553 0.5172
> 
> # Evaluate weighted metrics at each threshold
> m3_metrics_list <- lapply(thr_tbl_m3$Threshold, function(thr) {
+   with(test_df, weighted_metrics(pred3, CVD_num, WTMEC2YR, thr = thr))
+ })
> metrics_m3_cal <- dplyr::bind_rows(m3_metrics_list) |>
+   dplyr::mutate(Index = thr_tbl_m3$Index, .before = 1)
> 
> cat("\n[MODEL 3: Weighted metrics at candidate thresholds]\n"); print(metrics_m3_cal)

[MODEL 3: Weighted metrics at candidate thresholds]
# A tibble: 4 × 10
  Index   Threshold Weighted_Accuracy Weighted_Recall Weighted_Specificity Weighted_Precision Weighted_NPV LR_Positive LR_Negative
  <chr>       <dbl>             <dbl>           <dbl>                <dbl>              <dbl>        <dbl>       <dbl>       <dbl>
1 Youden…    0.145              0.815           0.722                0.824              0.28         0.969        4.10       0.338
2 Rule-i…    0.365              0.908           0.28                 0.968              0.454        0.934        8.74       0.744
3 Rule-o…    0.0399             0.579           0.934                0.545              0.163        0.989        2.06       0.121
4 Closes…    0.145              0.815           0.722                0.824              0.28         0.969        4.10       0.338
# ℹ 1 more variable: Weighted_F2 <dbl>
> 
> 
> 
> # Single view for thresholds + full metrics for Model 3
> m3_summary <- thr_tbl_m3 |>
+   dplyr::rename(Cutoff = Threshold) |>
+   dplyr::left_join(
+     dplyr::rename(metrics_m3_cal, Cutoff = Threshold),
+     by = c("Index", "Cutoff")
+   ) |>
+   dplyr::select(
+     Index, Cutoff,
+     Weighted_Accuracy, Weighted_Recall, Weighted_Specificity,
+     Weighted_Precision, Weighted_NPV, LR_Positive, LR_Negative, Weighted_F2
+   ) |>
+   dplyr::mutate(dplyr::across(where(is.numeric), ~ round(.x, 3)))
> 
> cat("\n[MODEL 3: Calibrated thresholds + full metrics]\n"); print(m3_summary)

[MODEL 3: Calibrated thresholds + full metrics]
                 Index Cutoff Weighted_Accuracy Weighted_Recall Weighted_Specificity Weighted_Precision Weighted_NPV LR_Positive
1             Youden J  0.145             0.815           0.722                0.824              0.280        0.969       4.098
2  Rule-in (Spec≥0.95)  0.365             0.908           0.280                0.968              0.454        0.934       8.744
3 Rule-out (Sens≥0.95)  0.040             0.579           0.934                0.545              0.163        0.989       2.055
4     Closest to (0,1)  0.145             0.815           0.722                0.824              0.280        0.969       4.098
  LR_Negative Weighted_F2
1       0.338       0.549
2       0.744       0.303
3       0.121       0.480
4       0.338       0.549
> 
> # ===============================
> # 8) Train-set ROC/AUC (unweighted)
> # ===============================
> train_df$pred1_tr <- predict(model1, type = "response")
> train_df$pred2_tr <- predict(model2, type = "response")
> train_df$pred3_tr <- predict(model3, type = "response")
> 
> 
> roc1_train <- pROC::roc(train_df$CVD, train_df$pred1_tr, levels = c("No","Yes"),
+                         weights = train_df$WTMEC2YR, quiet = TRUE)
> roc2_train <- pROC::roc(train_df$CVD, train_df$pred2_tr, levels = c("No","Yes"),
+                         weights = train_df$WTMEC2YR, quiet = TRUE)
> roc3_train <- pROC::roc(train_df$CVD, train_df$pred3_tr, levels = c("No","Yes"),
+                         weights = train_df$WTMEC2YR, quiet = TRUE)
> 
> 
> results_compare <- data.frame(
+   Model = c("Model 1","Model 2","Model 3"),
+   Train_AUC = c(auc(roc1_train), auc(roc2_train), auc(roc3_train)),
+   Test_AUC  = c(auc1_test,       auc2_test,       auc3_test)
+ )
> 
> results_compare_print <- dplyr::mutate(results_compare,
+                                        dplyr::across(where(is.numeric), ~ round(.x, 3)))
> 
> cat("\n[AUC TRAIN vs TEST]\n"); print(results_compare_print)

[AUC TRAIN vs TEST]
    Model Train_AUC Test_AUC
1 Model 1     0.592    0.552
2 Model 2     0.805    0.785
3 Model 3     0.828    0.825
> 
> 
> # ===============================
> # 9) Plot Test ROC curves
> # ===============================
> # All 3 models 
> plot(
+   roc1_test,
+   col = "red",
+   main = "Test ROC (2021–2023) — models trained on 2017–2020",
+   xlab = "1 - Specificity",
+   ylab = "Sensitivity",
+   lwd = 2,
+   legacy.axes = TRUE  # flip x-axis direction to 0 → 1
+ )
> lines(roc2_test, col = "blue", lwd = 2)
> lines(roc3_test, col = "green", lwd = 2)
> legend("bottomright",
+        legend = c(paste("Model 1:", round(auc1_test, 3)),
+                   paste("Model 2:", round(auc2_test, 3)),
+                   paste("Model 3:", round(auc3_test, 3))),
+        col = c("red","blue","green"), lty = 1, lwd = 2, cex = 0.85)
> 
> 
> # Weighted precision–recall curve for Model 3
> wpr_auc <- PRROC::pr.curve(
+   scores.class0 = test_df$pred3[test_df$CVD_num == 0],
+   scores.class1 = test_df$pred3[test_df$CVD_num == 1],
+   weights.class0 = test_df$WTMEC2YR[test_df$CVD_num == 0],
+   weights.class1 = test_df$WTMEC2YR[test_df$CVD_num == 1],
+   curve = FALSE # took up too much memory
+ )$auc.integral
> cat("Weighted PR-AUC (Model 3):", round(wpr_auc, 3), "\n")
Weighted PR-AUC (Model 3): 0.811 
> 
> 
> # ===============================
> # 10) Continuous IBI and model with spline
> # ===============================
> 
> # Continuous IBI
> model3_cont <- svyglm(CVD ~ IBI + Age + Gender + Ethnicity + Education +
+                         Smoking_status + Alcohol + BMI + Diabetes +
+                         TOTAL_CHOLESTEROL + HDL_CHOLESTEROL + Hypertension,
+                       design = train_design, family = quasibinomial("logit"))
> print(summary(model3_cont))

Call:
svyglm(formula = CVD ~ IBI + Age + Gender + Ethnicity + Education + 
    Smoking_status + Alcohol + BMI + Diabetes + TOTAL_CHOLESTEROL + 
    HDL_CHOLESTEROL + Hypertension, design = train_design, family = quasibinomial("logit"))

Survey design:
svydesign(id = ~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTMEC2YR, 
    nest = TRUE, survey.lonely.psu = "adjust", data = train_df)

Coefficients:
                              Estimate Std. Error t value Pr(>|t|)    
(Intercept)                  -5.674095   0.510110 -11.123 1.06e-05 ***
IBI                           0.001427   0.001523   0.937 0.379919    
Age                           0.071162   0.004221  16.861 6.32e-07 ***
GenderFemale                 -0.170570   0.122987  -1.387 0.208031    
EthnicityHispanic            -0.612287   0.184858  -3.312 0.012903 *  
EthnicityNon-Hispanic Black  -0.054277   0.137575  -0.395 0.704937    
EthnicityOther Muti-Racial   -0.006271   0.269890  -0.023 0.982112    
EducationBelow High School    0.665081   0.228369   2.912 0.022586 *  
EducationHigh School          0.292919   0.151933   1.928 0.095210 .  
Smoking_statusCurrent Smoker  0.599819   0.159585   3.759 0.007088 ** 
Smoking_statusFormer Smoker   0.325931   0.194623   1.675 0.137908    
AlcoholFrequent Drinker      -0.078969   0.129370  -0.610 0.560874    
AlcoholOccasional Drinker    -0.089733   0.094803  -0.947 0.375404    
BMI                           0.024151   0.007696   3.138 0.016423 *  
DiabetesYes                   0.767660   0.139105   5.519 0.000889 ***
DiabetesBorderline            0.075750   0.290281   0.261 0.801638    
TOTAL_CHOLESTEROL            -0.008452   0.001517  -5.572 0.000841 ***
HDL_CHOLESTEROL              -0.006627   0.005176  -1.280 0.241242    
HypertensionYes               0.342974   0.127497   2.690 0.031084 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for quasibinomial family taken to be 0.8560169)

Number of Fisher Scoring iterations: 6

> # Splines for IBI
> model3_spline <- svyglm(CVD ~ ns(IBI, df = 3) + Age + Gender + Ethnicity + Education +
+                           Smoking_status + Alcohol + BMI + Diabetes +
+                           TOTAL_CHOLESTEROL + HDL_CHOLESTEROL + Hypertension,
+                         design = train_design, family = quasibinomial("logit"))
> summary(model3_cont)

Call:
svyglm(formula = CVD ~ IBI + Age + Gender + Ethnicity + Education + 
    Smoking_status + Alcohol + BMI + Diabetes + TOTAL_CHOLESTEROL + 
    HDL_CHOLESTEROL + Hypertension, design = train_design, family = quasibinomial("logit"))

Survey design:
svydesign(id = ~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTMEC2YR, 
    nest = TRUE, survey.lonely.psu = "adjust", data = train_df)

Coefficients:
                              Estimate Std. Error t value Pr(>|t|)    
(Intercept)                  -5.674095   0.510110 -11.123 1.06e-05 ***
IBI                           0.001427   0.001523   0.937 0.379919    
Age                           0.071162   0.004221  16.861 6.32e-07 ***
GenderFemale                 -0.170570   0.122987  -1.387 0.208031    
EthnicityHispanic            -0.612287   0.184858  -3.312 0.012903 *  
EthnicityNon-Hispanic Black  -0.054277   0.137575  -0.395 0.704937    
EthnicityOther Muti-Racial   -0.006271   0.269890  -0.023 0.982112    
EducationBelow High School    0.665081   0.228369   2.912 0.022586 *  
EducationHigh School          0.292919   0.151933   1.928 0.095210 .  
Smoking_statusCurrent Smoker  0.599819   0.159585   3.759 0.007088 ** 
Smoking_statusFormer Smoker   0.325931   0.194623   1.675 0.137908    
AlcoholFrequent Drinker      -0.078969   0.129370  -0.610 0.560874    
AlcoholOccasional Drinker    -0.089733   0.094803  -0.947 0.375404    
BMI                           0.024151   0.007696   3.138 0.016423 *  
DiabetesYes                   0.767660   0.139105   5.519 0.000889 ***
DiabetesBorderline            0.075750   0.290281   0.261 0.801638    
TOTAL_CHOLESTEROL            -0.008452   0.001517  -5.572 0.000841 ***
HDL_CHOLESTEROL              -0.006627   0.005176  -1.280 0.241242    
HypertensionYes               0.342974   0.127497   2.690 0.031084 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for quasibinomial family taken to be 0.8560169)

Number of Fisher Scoring iterations: 6

> summary(model3_spline)

Call:
svyglm(formula = CVD ~ ns(IBI, df = 3) + Age + Gender + Ethnicity + 
    Education + Smoking_status + Alcohol + BMI + Diabetes + TOTAL_CHOLESTEROL + 
    HDL_CHOLESTEROL + Hypertension, design = train_design, family = quasibinomial("logit"))

Survey design:
svydesign(id = ~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTMEC2YR, 
    nest = TRUE, survey.lonely.psu = "adjust", data = train_df)

Coefficients:
                              Estimate Std. Error t value Pr(>|t|)    
(Intercept)                  -5.828428   0.528083 -11.037 0.000106 ***
ns(IBI, df = 3)1              3.641382   2.114212   1.722 0.145623    
ns(IBI, df = 3)2             -1.192723   2.345518  -0.509 0.632730    
ns(IBI, df = 3)3             -7.530478   5.743568  -1.311 0.246800    
Age                           0.070145   0.004174  16.807 1.36e-05 ***
GenderFemale                 -0.192457   0.120954  -1.591 0.172446    
EthnicityHispanic            -0.599677   0.180790  -3.317 0.021078 *  
EthnicityNon-Hispanic Black  -0.021992   0.140061  -0.157 0.881373    
EthnicityOther Muti-Racial    0.034748   0.266569   0.130 0.901368    
EducationBelow High School    0.655518   0.226132   2.899 0.033837 *  
EducationHigh School          0.275259   0.150652   1.827 0.127239    
Smoking_statusCurrent Smoker  0.571161   0.156389   3.652 0.014715 *  
Smoking_statusFormer Smoker   0.326808   0.192622   1.697 0.150530    
AlcoholFrequent Drinker      -0.056316   0.128787  -0.437 0.680150    
AlcoholOccasional Drinker    -0.084670   0.096224  -0.880 0.419174    
BMI                           0.015941   0.007440   2.143 0.085019 .  
DiabetesYes                   0.747460   0.142326   5.252 0.003321 ** 
DiabetesBorderline            0.090243   0.294548   0.306 0.771666    
TOTAL_CHOLESTEROL            -0.009044   0.001603  -5.643 0.002424 ** 
HDL_CHOLESTEROL              -0.004809   0.005065  -0.949 0.386070    
HypertensionYes               0.332993   0.125316   2.657 0.045034 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for quasibinomial family taken to be 0.8615713)

Number of Fisher Scoring iterations: 6

> 
> # Compare TEST AUCs for these
> test_df$pred3_cont   <- predict(model3_cont,   newdata=test_df, type="response")
> test_df$pred3_spline <- predict(model3_spline, newdata=test_df, type="response")
> 
> roc3_cont   <- pROC::roc(test_df$CVD, test_df$pred3_cont,   levels=c("No","Yes"), weights=test_df$WTMEC2YR, quiet=TRUE)
> roc3_spline <- pROC::roc(test_df$CVD, test_df$pred3_spline, levels=c("No","Yes"), weights=test_df$WTMEC2YR, quiet=TRUE)
> 
> cat("\n[Model 3 variants — AUC on TEST]\n")

[Model 3 variants — AUC on TEST]
> cat("  IBI continuous: ", round(pROC::auc(roc3_cont),   3), "\n")
  IBI continuous:  0.829 
> cat("  IBI spline:     ", round(pROC::auc(roc3_spline), 3), "\n")
  IBI spline:      0.827 
> 
> 
> 
> 
> # 10b) IBI as binary (top ha;f vs bottom half)
> 
> # Create binary on train and test using existing quartiles
> train_df$IBI_high <- as.integer(train_df$IBI >= ibi_quartiles[3])
> test_df$IBI_high  <- as.integer(test_df$IBI  >= ibi_quartiles[3])
> 
> train_design <- svydesign(
+   id = ~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTMEC2YR,
+   nest = TRUE, survey.lonely.psu = "adjust",
+   data = train_df
+ )
> 
> # Fit with IBI_high
> model3_binary <- svyglm(
+   CVD ~ IBI_high + Age + Gender + Ethnicity + Education +
+     Smoking_status + Alcohol + BMI + Diabetes +
+     TOTAL_CHOLESTEROL + HDL_CHOLESTEROL + Hypertension,
+   design = train_design, family = quasibinomial("logit")
+ )
> cat("\n[FIT] Model 3 (IBI binary: Q3-4 vs Q1-2)\n"); print(summary(model3_binary))

[FIT] Model 3 (IBI binary: Q3-4 vs Q1-2)

Call:
svyglm(formula = CVD ~ IBI_high + Age + Gender + Ethnicity + 
    Education + Smoking_status + Alcohol + BMI + Diabetes + TOTAL_CHOLESTEROL + 
    HDL_CHOLESTEROL + Hypertension, design = train_design, family = quasibinomial("logit"))

Survey design:
svydesign(id = ~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTMEC2YR, 
    nest = TRUE, survey.lonely.psu = "adjust", data = train_df)

Coefficients:
                              Estimate Std. Error t value Pr(>|t|)    
(Intercept)                  -5.583721   0.511089 -10.925 1.19e-05 ***
IBI_high                      0.326048   0.125443   2.599 0.035468 *  
Age                           0.070433   0.004171  16.885 6.26e-07 ***
GenderFemale                 -0.191036   0.124362  -1.536 0.168388    
EthnicityHispanic            -0.603183   0.183690  -3.284 0.013418 *  
EthnicityNon-Hispanic Black  -0.036920   0.141543  -0.261 0.801725    
EthnicityOther Muti-Racial    0.008651   0.265289   0.033 0.974897    
EducationBelow High School    0.664382   0.228242   2.911 0.022633 *  
EducationHigh School          0.282589   0.150979   1.872 0.103417    
Smoking_statusCurrent Smoker  0.569021   0.151497   3.756 0.007112 ** 
Smoking_statusFormer Smoker   0.316759   0.192021   1.650 0.143010    
AlcoholFrequent Drinker      -0.072536   0.126311  -0.574 0.583762    
AlcoholOccasional Drinker    -0.084457   0.098218  -0.860 0.418320    
BMI                           0.017781   0.007337   2.424 0.045853 *  
DiabetesYes                   0.763607   0.140933   5.418 0.000989 ***
DiabetesBorderline            0.099983   0.297680   0.336 0.746809    
TOTAL_CHOLESTEROL            -0.008830   0.001538  -5.741 0.000705 ***
HDL_CHOLESTEROL              -0.005385   0.005023  -1.072 0.319292    
HypertensionYes               0.340528   0.126882   2.684 0.031365 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for quasibinomial family taken to be 0.8527679)

Number of Fisher Scoring iterations: 6

> 
> 
> # Predict on train data
> train_df$pred3_binary <- predict(model3_binary, type = "response")
> 
> roc3_binary_train <- pROC::roc(
+   train_df$CVD, train_df$pred3_binary,
+   levels = c("No","Yes"), quiet = TRUE
+ )
> auc3_binary_train <- pROC::auc(roc3_binary_train)
> 
> 
> 
> # Predict & ROC/AUC on TEST
> test_df$pred3_binary <- predict(model3_binary, newdata = test_df, type = "response")
> 
> roc3_binary <- pROC::roc(
+   test_df$CVD, test_df$pred3_binary,
+   levels = c("No","Yes"),
+   weights = test_df$WTMEC2YR, quiet = TRUE
+ )
> auc3_binary <- pROC::auc(roc3_binary)
> 
> cat("\n[Model 3 (binary IBI) — AUC on TEST]\n")

[Model 3 (binary IBI) — AUC on TEST]
> cat("  IBI binary (Q4 vs others): ", round(auc3_binary, 3), "\n")
  IBI binary (Q4 vs others):  0.827 
> 
> 
> # Compare continuous, spline, and binary models
> results_compare2 <- results_compare |>
+   dplyr::bind_rows(tibble::tibble(
+     Model = "Model 3 (IBI binary)",
+     Train_AUC = as.numeric(auc3_binary_train),
+     Test_AUC  = as.numeric(auc3_binary)
+   ))
> 
> 
> results_compare_print <- results_compare2 |>
+   dplyr::mutate(dplyr::across(where(is.numeric), ~ round(.x, 3)))
> 
> cat("\n[AUC TRAIN vs TEST] (updated with binary IBI)\n"); print(results_compare_print)

[AUC TRAIN vs TEST] (updated with binary IBI)
                 Model Train_AUC Test_AUC
1              Model 1     0.592    0.552
2              Model 2     0.805    0.785
3              Model 3     0.828    0.825
4 Model 3 (IBI binary)     0.827    0.827
> 
> # ===============================
> # 11)  Dose–response for IBI
> # ===============================
> 
> # Use the trend model (numeric quartile score) to get fitted probabilities
> gg_ibi_quart <- ggpredict(
+   model3_Q_Num,
+   terms = "IBI_QuartileNum [1:4]"   # predicts for quartiles 1, 2, 3, 4
+ )
> summary(model3)

Call:
svyglm(formula = CVD ~ IBI_Category + Age + Gender + Ethnicity + 
    Education + Smoking_status + Alcohol + BMI + Diabetes + TOTAL_CHOLESTEROL + 
    HDL_CHOLESTEROL + Hypertension, design = train_design, family = binomial("logit"))

Survey design:
svydesign(id = ~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTMEC2YR, 
    nest = TRUE, survey.lonely.psu = "adjust", data = train_df)

Coefficients:
                              Estimate Std. Error t value Pr(>|t|)    
(Intercept)                  -5.760154   0.512440 -11.241 9.73e-05 ***
IBI_CategoryQ2                0.411489   0.186062   2.212  0.07795 .  
IBI_CategoryQ3                0.543978   0.166330   3.270  0.02219 *  
IBI_CategoryQ4                0.639112   0.195352   3.272  0.02216 *  
Age                           0.069875   0.004301  16.248 1.61e-05 ***
GenderFemale                 -0.201157   0.124854  -1.611  0.16807    
EthnicityHispanic            -0.612506   0.186667  -3.281  0.02192 *  
EthnicityNon-Hispanic Black  -0.027628   0.141112  -0.196  0.85248    
EthnicityOther Muti-Racial    0.030552   0.262396   0.116  0.91184    
EducationBelow High School    0.671820   0.224326   2.995  0.03028 *  
EducationHigh School          0.277627   0.150269   1.848  0.12394    
Smoking_statusCurrent Smoker  0.569314   0.153397   3.711  0.01384 *  
Smoking_statusFormer Smoker   0.320015   0.193136   1.657  0.15843    
AlcoholFrequent Drinker      -0.075121   0.126088  -0.596  0.57726    
AlcoholOccasional Drinker    -0.091408   0.094863  -0.964  0.37951    
BMI                           0.016196   0.007353   2.203  0.07881 .  
DiabetesYes                   0.769041   0.149982   5.128  0.00368 ** 
DiabetesBorderline            0.083039   0.295404   0.281  0.78989    
TOTAL_CHOLESTEROL            -0.009033   0.001579  -5.721  0.00228 ** 
HDL_CHOLESTEROL              -0.004495   0.005056  -0.889  0.41471    
HypertensionYes               0.335455   0.127043   2.640  0.04595 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 0.8631825)

Number of Fisher Scoring iterations: 6

> 
> # Optional: relabel quartiles nicely for the plot
> gg_ibi_quart$x <- factor(gg_ibi_quart$x, labels = c("Q1", "Q2", "Q3", "Q4"))
> 
> # Build the ggplot object
> p_ibi_quart <- plot(gg_ibi_quart) +
+   theme_minimal() +
+   labs(
+     title = "Predicted Probability of CVD by IBI Quartile",
+     x = "IBI Quartile",
+     y = "Predicted Probability of CVD"
+   ) +
+   geom_point(size = 3) +
+   geom_line(group = 1, linewidth = 1) +
+   ylim(0, NA) +
+   theme(
+     plot.title = element_text(face = "bold"),
+     axis.title = element_text(size = 12),
+     axis.text = element_text(size = 11)
+   )
Scale for y is already present.
Adding another scale for y, which will replace the existing scale.
> 
> # Display
> print(p_ibi_quart)
> 